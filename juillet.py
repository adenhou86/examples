| #      | Slide thumb                                            | What to say (≈‑30 sec each)                                                                                                                                                                                                                                                                |
| ------ | ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **1**  | *Section break – “Single‑Document Q\&A Optimized RAG”* | “Let’s start with how we took vanilla RAG and tuned it for a single filing. You’ll see three key upgrades: layout‑aware parsing, hybrid retrieval with re‑ranking, and post‑retrieval context filtering for compliance.”                                                                   |
| **2**  | *Objectives list*                                      | “Our single‑doc engine supports three day‑to‑day analyst tasks: **1)** pinpoint numeric facts – e.g. cost of revenues; **2)** narrative summaries like ongoing litigation; and **3)** accurate table extraction, even from messy PDFs. Together those cover > 70 % of research questions.” |
| **3**  | *Basic RAG diagram*                                    | “Here’s the baseline: user query → embeddings → vector search → chunks sent to the LLM. It’s fast but blind to PDF layout, so footers, headers, and tables often creep into answers.”                                                                                                      |
| **4**  | *Optimised single‑doc diagram*                         | “We injected intelligence at every stage: computer‑vision models find tables, metadata gets added to chunk vectors, sparse + dense retrieval is re‑ranked, and we filter contexts before prompting the LLM. Net result: +18 pp accuracy with only +0.5 s latency.”                         |
| **5**  | *North America Credit / BlackRock demo page*           | “Proof we can handle visually complex sources: charts, pie graphs, images, and text all end up in a coherent answer with traceable citations. That’s the computer‑vision and table‑handling boxes you saw on the previous slide at work.”                                                  |
| **6**  | *Section break – “Agentic Multi‑Documents RAG”*        | “Now we widen the lens: answering cross‑document questions like ‘compare JPM vs GS revenue growth and risk exposure over five years’. We add an agent layer, metadata routing, and dynamic retrieval strategies.”                                                                          |
| **7**  | *Text block – query categories*                        | “First, the agent decomposes intent: entity names, time windows, sectors, even infers the right data sources—EDGAR for filings, Fitch for ratings, Refinitiv for call transcripts. That shrink‑wraps the search space before we touch the vector DB.”                                      |
| **8**  | *Desk / document‑type catalogue*                       | “Because our 25‑million‑doc store spans desks—LevLoans, LevFin, MSIM—we tag each document with its desk and type at ingest. The decomposition agent uses this map to hit only the desks and doc types that matter for the question.”                                                       |
| **9**  | *Blue table – end‑to‑end flow*                         | “Here’s the full pipeline: **1)** decomposition agent, **2)** metadata filter via DocCentral / ResearchCentral, **3)** choose combined vs separate retrieval, **4)** LLM synthesis, **5)** answer back to the user. All steps and latencies are logged for audit.”                         |
| **10** | *Amazon example – routing statement*                   | “A concrete example: the agent turns ‘Amazon operating margins for Q1 & Q3 2024’ into a routing plan—MSIM desk, 10‑K, 8‑K press releases, MDA section—plus explicit filters. That precision is why multi‑doc answers stay accurate and on‑budget.”                                         |
